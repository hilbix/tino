> Leser lesen nur die Überschrift und gehen dann zum nächsten Artikel weiter. Steht in der Überschrift eine Lüge ist das ein Fall für Lügenpresse.
>
> Wenn die Überschrift im Text irgendwo klein richtigestellt wird, sich also der Artikel selbst widerspricht, bleibt es dennoch eine Lüge.
>
> Deshalb wird es in Zukunft gar nichts mehr anderes geben als Lügenpresse.
> Oder anders gesagt:
>
> Jeder wird in seiner eigenen Filterblase glücklich werden können.
> Dank geeigneter Deepfakes können wir gar nicht mehr entscheiden, was wahr oder falsch ist!
>
> Ob das gut oder schlecht ist?  Entscheidet selbst!


# Wir haben den Kampf gegen Deepfakes schon verloren

Inzwischen gibt es AIs, die Deepfakes erkennen können.  Also alles gut?

Nein, denn die wirkliche Dimension des Problems habt ihr überhaupt noch nicht verstanden!


## AI generierte Bilder können wir erkennen

Wir haben da ein kleines Rennen:

- Die erste AI erzeugt ein Bild
- Eine andere AI erkennt das Bild als Fake
- Dann trainiert man die erste AI mit der anderen AI so lange, bis sie Bilder generiert, die die AI nimmer erkennen kann.
- Dann trainiert man eine andere AI halt darauf, diese neuen Bilder zu erkennen

Wash, Rinse, Repeat.

Und wer gewinnt das Rennen?

- Kurzfristig die erstern AIs
- Langfristig die erkennenden AIs

Denn ein Bild ist statisch.  Es wird also verdammt schwer werden, ein Bild durch eine AI konstruieren zu lassen,
dass es nicht irgendwann als genau das erkannt wird.

Wenn wir aber auf jeden Fall irgendwann erkennen können, ob ein Bild per AI generiert wurde,
dann sind Deepfakes ja letztendlich wohl kein Problem, oder?

Doch, denn das ist ein Fehlschluss.  Wir können nur erkennen, ob ein Bild von einer AI generiert wurde.
Ob das Bild ein Deepfake ist oder nicht können wir dadurch nicht herausfinden.


## Digitalfotographie, bisher

Früher, da gab es nur analoge Fotografie.  Da gab es viele Möglichkeiten, zu prüfen, ob ein Bild original war.
Z.B. konnte man auf dem Negativ die entsprechenden Spuren der Kamera und des Entwicklungsprozesses finden,
und so darauf schließen, ob etwas authentisch ist.  U.v.m.  Die Freiheitsgrade hinter analogen Prozessen sind
derart hoch, dass man mit dieser schier unüberwindlichen Menge an Daten nicht alles richtig machen konnte.

Und so konnte man Bilder als Fake entlarven.  Wenn man denn wollte.  Und die notwendie Expertise hatte.

Heute aber ist alles digital.  Software nimmt die Daten vom Fotosensor ab und speichert sie.
Dabei kommen nur wenig Fehler vor, diese kann man zwar finden, aber es ist ein leichtes,
genau diese gleichen Fehler vorzutäuschen.  Also z.B. den Pixeldefekt eines Sensors,
den die Software natürlich "unsichtbar" ausbügelt, damit die Kamera kein Gewährleistungsfall wird.
Usw. etc. pp.

Hat man die genauen Parameter der Kamera, kann man sie emulieren.  Nicht simulieren sondern emulieren.
Dabei wird nur simuliert, was der emulierte Sensor sieht.

> Simulation und Emulation sind grundsätzlich verschieden.
> Technisch sind sie zwar sehr ähnlich, aber eine Simulation kann die Realität nicht verändern.
> Ein simulierter Prozessor kann zwar etwas berechnen und so ein Ergebnis liefern, aber nur
> ein emulierter Prozessor kann einen defekten realen Prozessor ersetzen.
>
> Auf einer Von-Neumann-Hardware können wir Quantensimulationen (Vielweltenansatz) laufen lassen.
> Eine Quantenemulation können wir so aber nicht herstellen, dazu braucht es zwingend einen Quantencomputer.

Der Output einer so emulierten Kamera ist vom Output des Originals nicht mehr zu unterscheiden.
Einzig unterscheiden kann man Bilder deshalb nur noch an den Pixelrohdaten, die vom Sensor kommen.
Und das wäre dann so etwas wie das RAW-Format.

Aus dem RAW-Format kann man schon sehr viel forensische Informationen erhalten.
Deshalb ist es ja so unheimlich beliebt, weil man Software nachschalten kann,
die dann die eigentliche Bildverbesserung macht.

Wenn man am nachbearbeiteten Bild vielleicht auch nicht mehr dessen Authentizität
erkennen kann, am RAW-Bild kann man das meistens noch.  Das sollte selbst dann noch funktionieren,
wenn man einen Fake hochauflösend projeziert und dann mit einer Digitalkamera aufnimmt.
Oder wenn man dies mit einer emulierten Kamera tut, der Fake also die simulierten Sensordaten darstellt.

Also ist doch alles gut!  Oder etwa doch nicht?

## Digitalfotographie, jetzt

Wer glaubt, ein Handy mit einer perfekten Kamera in einer Auflösung von 100 Megapixeln oder mehr zu haben,
der glaubt das auch wirlich nur.

Ja, die in das Handy verbauten Kamerasensoren haben rein physikalisch und technisch vielleicht tatsächlich
diese Auflösung, aber das täuscht gleich in mehrerer Hinsicht:

- Die Sensoren haben verschiedenen Auflösungen.
- Die Sensoren haben jeweils eine eigene Optik
- Und die Sensoren haben Defekte, die sie nicht mehr zeigen

Letzteres liegt am Produktionsprozess.  Wenn wir über 100 Millionen Transistoren (eigentlich sind das
glaube ich Fets, aber nennen wir sie mal Transistoren) auf ein lichtstarkes Die baue, und erwarte,
dass all diese Transistoren auch wirklich funktioneren, dann habe ich eine deutlich geringere Ausbeute.
Zwar kosten Handys schon teils 2000 EUR, aber wer hat soetwas?

Die Mehrzahl der Leute hat Handys um 200 EUR.  Und die haben ähnliche Sensoren verbaut.  Massenhaft.

Und diese Sensoren lügen bereits.  Sie sind so konstruiert, dass sie fehlerhafte Zellen erkennen,
und dann deren Wert errechnen (oder die nachgeschaltete Software tut das).  Damit keine unschönen
Pixel im Bild landen.  Damit niemand meckert.  Denn den einen oder anderen kaputten Pixel könnte man sehen,
aber den "reparierten" Pixel erkennt kein Mensch.  Software schon.  Menschen nicht.

viele Billighandys haben inzwischen 3 Kameras eingebaut.  Die nachgeschaltete Software mixt diese 3 Bilder so,
dass man hinterher ein perfektes Bild herausbekommt.  Ein Bild, bei dem man hinterher sogar Dinge wie
den Fokus verändern kann.  Nicht wirklich, aber das kann die Software zu einem sehr hohen Maß aus den
Rohdaten auf optisch korrekte Weise herausrechnen.  (Auf ähnliche Weise wurde auch die Sehunschärfe von Hubble
anfangs herausgerechnet, bis er seine Brille bekam.)

Optisch korrekt ist aber nicht unbedingt wirklich korrekt.  Die Algorithmen sind dabei auf den Menschen
getrimmt.  Das bedeutet, für unser Auge soll das perfekt erscheinen.  Nicht aber so für die AI.

Die AI erkennt solche Makel sofort.  Aber die AI kann erkennen, ob es sich um "Standardkorrekturen" handelt
oder um Deepfakes.  Und so kann sie die Bilder ggf. immer noch als authentisch einstufen.

Aber das Problem ist:  Wer fotographiert schon immer RAW?  Ich habe ein gutes Outood-Handy und habe versucht,
RAW-Dateien zu speichern.  Geht nicht.  Ich kann nur zwischen JPG und HEIF wählen.  HEIF ist zwar
(meineserachtens) deutlich besser als JPG, aber RAW ist das trotzdem nicht.

Und genau das führt nun zu dem Problem:  An RAW kommt man gar nicht mehr ran!
Das bedeutet, man hat für die Fake-Erkennung deutlich weniger Informationen zur Hand.


## Digitalfotographie, Zukunft

Noch haben wir etwas Informationen, die die Fake-Erkennung ermöglicht.  Aber die verschwindet zusehends.

Mein Handy hat das noch nicht, aber die akuten Handys haben sie meistens schon:

Eine AI, die Bilder vollautomatisch optimiert.

Das geht auch schlecht anders.  Die Anzahl der Sensoren nimmt ständig zu.  Kein Nutzer will sich lange
mit Nachbearbeitung aufhalten.  Es reicht, wenn man mit Hilfe einer AI das Bild passend nachbearbeiten kann.

Was allerdings gleich eine doppelte Marketinglüge darstellt.  Denn da ist gar keine AI am Werk!

Die Bildverarbeitung ist immer noch fest vorgegeben.  Es kommen zwar Algorithmen aus dem AI-Umfeld zum Einsatz,
dies sind aber statische neuronale Netzwerke, reduziert auf einen schnellen Algorithmus,
der dem Nutzer nur vorgaukelt, perfekt zu sein.  Aber das reicht.  Menschliche Augen sind ja sowas von ungenau.

Die zweite Marketinglüge ist allerdings eine tatsächlich verschwiegene AI, die Defekte in den Sensoren ausgleicht.
Das tut sie nicht mehr mit billigen Algorithmen.  Denn das würde u.U. auffallen.
Diese AI lernt sogar in einem gewissen Maß, denn die Pixelfehler könnten ja zunehmen.
Was bei den Nutzern einen schlechten Eindruck erwecken würde.

Nope.  Von dieser AI soll niemand etwas wissen.  Sie ist Teil der Firmware des Handys, also nicht des
Betriebssystems selbst, sondern schon heute eine Art Treiber.  Und das wird sich nicht ändern.

Im Gegenteil.  Diese AI wird immer mächtiger und mächtiger werden.  Sie halten da nur mit der Hardwareentwicklung
mit.  Neuronale Chips in Handys?  Klar kommen die.  Oder sind schon drin.

Neuronale Chips bedeutet jetzt nicht ein Hirn-Computerinterface, sondern lediglich, wie so eine AI aufgebaut ist.
Ziel ist dabei, den Energieeinsatz zu minimieren.

Einige heutige Handys erlauben bereits den Einsatz der Kamera bevor man den Auslöser drückt.
Ist auch ganz logisch, das Betriebssystem ist oft einfach nicht schnell genug und die Datenmenge
einer Kamera ständig zu verarbeiten würde zu viel Energie verbrauchen.

Wo steckt also diese ganze Logik?  In der Hardware selbst, natürlich.  Ist auch besser so,
denn so ein Kamera-Chip ist extrem groß.  Je größer, desto mehr Licht.  Aber dadurch ist
sowieso viel Fläche ungenutzt, auf der man ziemlich viel Rechenleistung unterbringen kann,
ohne dass das Die viel mehr Strom braucht.  Weil was bei den Chips viel Strom braucht sind
die Ausgangstreiber, und die Ansteuerelektronik.  Im Chip selbst werden die Signale hingegen
nur vergleichsweise kleine Strecken transportiert und entsprechend wenig Ladung muss verschoben werden.

Und dieser Trend wird sich weiterhin fortsetzen.  Die Kamera-Chips werden bei gleichen Kosten
immer leistungsfähiger.  Was dabei aber als erstes schon auf der Strecke blieb ist,
an das RAW-Format heranzukommen.  Denn bei steigender Leistung steigt auch die Anforderung
and die Übertragung der Datenmenge, was immer aufwendiger wird (Chips entwickeln sich mindestens quadratisch,
Datenübertraungs-Bussysteme eher linear) und damit teuer wird.  Jedenfalls wenn man es brauchbar hält.

Und irgendwann wandert ein Großteil der AI in den Kamerachip.  Da ist also keine Software im Handy mehr dran,
etwas, das man vielleicht abschalten könnte, sondern da kommen gleich AI-verbesserte Daten aus dem Chip.

Keine Chance die AI zu kontrollieren.  Das tut der Hersteller.

Noch.

Bis sie dann auf den Trichter kommen, dass man die Firmware vom Kamerachip ja auch laden kann.
Was den Produktionsprozess vereinfacht.  Man muss die Module nicht fertig ausliefern,
sondern kann hinterher patchen.  Produktionsfehler ausbügeln.  Oder Optionen gegen Geld freischalten,
weil es oft billiger ist, man baut nur 1 Chip das alles kann, aber Premiumfunktionen kosten halt extra.

> Das ist oft gar kein böser Wille, sondern mal wieder dem Patentsystem geschuldet.
> Wenn mein Chip ein bestimmtes Patent unterstützt, dann ist Kohle fällig.
> Und wenn es das gar nicht in Hardware tut?  Dann auch, sofern man dies per Software machen kann.
>
> Die "Billigvariante" vom Chip (es ist physikalisch dasselbe Chip!) hat die Software nicht,
> dadurch muss man auch keine Patentgebühren dafür abführen.
> Die Premiumvariante hingegen muss die Patentgebühren ebenfalls erlösen.
> Beides ist dieselbe Hardware, evtl. unterscheidet sie sich nur in einem kleinen gesetzten Bit (FUSE)
> welches bei der Firmware dafür sorgt, ob sie kastriert läuft oder nicht.
>
> Ich sagte, das wäre kein böser Wille.  Ist es schon, nur nicht vom Hardwarehersteller selbst.
> Derartige Softwarepatente sind in der EU zwar verboten, aber nicht weltweit.
> Und der Hersteller will seine Hardware ja weltweit verkaufen können.

All das ist nichts neues und gibt es heute schon.  Oft aber wird es noch nicht benötigt.
Aber so war die Entwicklung der letzten 40 Jahre nuneinmal, und wird auch genau so weitergehen.

In der Zukunft sind somit alle Bilder mehr oder weniger von einer AI generiert.
Und den Nutzern ist das vollkommen egal, sie erhalten schöne Bilder.

## Fazit

Die Entwicklung wird voranschreiten.  Der Großteil aller Bilder wird in Zukunft deshalb von AIs generiert.

Die Deepfakes sowieso.  Aber auch die realen Fotos.

Die Deepfake-AI kann sich also hinter der Kamera-AI verstecken.  Eine reine Erkennung, ob es sich um
ein AI-generiertes Bild handelt oder nicht reicht damit nicht mehr aus, um Deepfakes zu erkennen.

Da aber die Deepfake-Erkennung nicht alle Kameramodelle und alle in den Kameras eingesetzten AIs kennen kann, - außer das wird gesetzlich geregelt und die müssten ihre Software offenlegen, was ganz sicher nicht passiert -
steht man nun vor einem Unentscheidbarkeitsproblem:

- Ist es ein Deep-Fake, der lediglich so tut als sei er eine unbekannte Kamera-AI
- Oder war das tatsächlich eine unbekannte Kamera-AI

Für die paar nächsten Jahre bleibt das noch entscheidbar.  Die Menge der letzteren Fotos werden entweder
so wenige sein, dass es nicht ins Gewicht fällt, oder so viele, dass man sie herausrechnen (trainieren) kann.

Aber auf lange Sicht wächst dieser Berg immer weiter an.  Irgendwann ist es dann soweit, dass eine
Deepfake-AI Bilder so gut herstellen kann, dass man nicht mehr entscheiden kann, war das eine Kamera-AI
oder ein Deepfake.

Interessanterweise wird das vor allem Bilder sein, die dann älter zu sein scheinen, denn diese
"unbekannten Kamera-AI-Bilder", deren Firmware man nicht kennt also nicht nachvollziehen kann,
sind dann vor allem älteren Datums, weil sie zu Hardware gehören, die schon lange nicht mehr
im Einsatz ist.  Wer so einen Deepfake ins Netz stellt würde also daran erkannt, dass er diese
Hardware vermutlich gar nicht mehr besitzt oder besitzen kann.

Aber das gilt nicht, wenn es "plötzlich aufgetauchte Bilder aus der etwas zurückliegenden Vergangenheit"
sind, die etwas "beweisen", und damit die Geschichtsschreibung umschreiben könnte.

Während derzeit erstellte Deepfakes also in Zukunft durch die Weiterentwicklung von Erkennungs-AIs
ganz sicher erkannt werden, werden in Zukunft auf die Vergangenheit gerichtete Deepfakes unmöglich
erkannt werden.  Weil man eben nicht mehr entscheiden kann, ob der Deepfake nur die Kamera emulierte,
oder es sich um die reale Kamera handelte.

Eine Blockchain könnte das lösen.  Nur müsste dann jedes Bild (bzw. dessen Fingerprint) in die
Blockchain eingetragen werden.  Das macht niemand.  Solche aufgetauchten Bilder würden außerdem
immer "rein zufällig" nicht in der Blockchain gespeichert (oder verwenden eine Kollision des Fingerprints,
die sich zufällig in der Blockchain befindet.  Das auszuschließen braucht Post-Quantum-Fingerprints,
welche zudem eine typische Bildnachbearbeitung überleben kann und doch den Inhalt des Bildes
authentisieren.  Ein IMHO unlösbares Problem).

Somit haben wir unsere zukünftige Vergangenheit bereits heute an das
[Ministerium für Wahrheit](https://de.wikipedia.org/wiki/1984_(Roman)) verloren.

Das wird sicher eine [Schöne neue Welt](https://de.wikipedia.org/wiki/Sch%C3%B6ne_neue_Welt).
Aber wohl nicht mehr zu meinen Lebzeiten.  Glücklicherweise?
