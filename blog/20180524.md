# sv3 und sv4

- Problem: Rechner hat selbsttätig rebootet
- Ursache: Stromausfall im RZ, Fehlerhafte USV
- Dauer: 11:00-19:10 (sv4) 11:00-09:00 (sv3)

Betroffene (wichtigere) VMs:

- Meine Homepage
- susi
- ts
- libepo Backend

Grund der Downtime:

- Die Maschine kommt nach einem Reboot von selbst hoch.
- sv4: Das ZFS-Filesystem muss derzeit manuell aktiviert werden.
  - Nachdem ZFS aktiviert ist müssen die VMs angefahren werden.
- sv3: Das ZFS-Filesystem kommt zwar von selbst hoch, aber der Webservice startet vor ZFS und fällt dadurch auf die Nase

Ich sollte das irgendwie automatisieren.

- Alle Probleme inkl. Probleme mit den VMs, siehe "Issues".

Das Gute im Schlechten:  Beide Hosts haben nun aktuelle Kernel, d. h. Meltdown-Fixes sind auch auf den Hosts aktiv.


## Protokoll

`/var/log/messages`:
```
May 24 08:51:11 sv4 rsyslogd-2007: action 'action 19' suspended, next retry is Thu May 24 08:51:41 2018 [try http://www.rsyslog.com/e/2007 ]
May 24 12:10:55 sv4 rsyslogd: [origin software="rsyslogd" swVersion="8.4.2" x-pid="3469" x-info="http://www.rsyslog.com"] start
May 24 12:10:55 sv4 kernel: [    0.000000] CPU0 microcode updated early to revision 0x22, date = 2017-01-27
```

`/var/log/syslog`:
```
May 24 11:03:01 sv4 CRON[5663]: (root) CMD (bin/minute.sh)
^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@May 24 12:10:55 sv4 rsyslogd: [origin software="rsyslogd" swVersion="8.4.2" x-pid="3469
" x-info="http://www.rsyslog.com"] start
```

`kern.log`:
```
May 24 08:55:25 sv4 kernel: [20001759.269479] UDP: bad checksum. From 60.191.0.242:58433 to 144.76.115.105:1080 ulen 49
May 24 12:10:55 sv4 kernel: [    0.000000] CPU0 microcode updated early to revision 0x22, date = 2017-01-27
```

`auth.log`:
```
May 24 11:03:10 sv4 CRON[5662]: pam_unix(cron:session): session closed for user root
^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@
May 24 12:10:56 sv4 sshd[4122]: Server listening on 0.0.0.0 port 22.
```

Sieht nach einem spontanen Reset aus.

## Issues

## sv4

- Kam selbständig hoch, aktivierte aber ZFS nicht.
- Aktionen dafür siehe `/root/README`

### VM susi.de

- Die Uhrzeit stimmt nicht.  Sie war 2h off.  Behauptet MESZ zu sein, ist aber UTC.  Aktion:
  - In `/etc/sysconfig/clock` eingetragen `HWCLOCK="-u"`, mal sehen ob das dann in Zukunft passt.
- NTP läuft, es hätte sich also von selbst gesynct, irgendwann, aber so ist das besser.

## sv3

- Kam vollständig hoch, aktivierte auch ZFS, aber nicht den Webservice.
- Diesen habe ich manuell durchgestartet (siehe `/root/README`), dann war alles OK

## Ursache

Die Ursache fand sich auf https://www.hetzner-status.de/

```
Störungsmeldung Stromversorgung/Netzwerk Datacenterpark Falkenstein 		
Typ: 	Störungsmeldung 	
Kategorien: 	Basis Infrastruktur
Start: 	24. Mai 2018 11:30:00 CEST
Ende: 	Unbekannt
Beschreibung: 	Aufgrund einer aktuellen Störung der Stromversorgung am Standort Falkenstein, gibt es derzeit Erreichbarkeitsprobleme einiger Kundenserver. Unsere Techniker arbeiten aktuell an der Ursachenanalyse und wir werden Sie entsprechend auf dem Laufenden halten.
Update: 	24. Mai 2018 12:10:00 CEST 	
	Update: Nach aktueller Kenntnislage gab es gegen 11:00 Uhr im DC7, DC10 + DC12 einen Stromausfall. Durch den einhergehenden Ausfall von Core-Netzwerktechnik am Standort Falkenstein, gab es durch die Spannungsunterbrechung einen kurzzeitgen Ausfall des gesamten Netzwerks im Datacenterpark Falkenstein
Update: 	24. Mai 2018 12:27:00 CEST 	
	Zwischenzeitlich konnte die Stromversorgung in den drei betroffenen Rechenzentren provisorisch (ohne USV-Absicherung) wiederhergestellt werden.

Ursache für den Ausfall war eine starke Spannungsabsenkung im örtlichen Stromnetz. Warum es trotz USV-Absicherung zu Ausfällen gekommen ist, wird noch geprüft. Aktuell hat die Wiederherstellung des gesicherten Betriebs oberste Priorität.

Unsere Techniker sind gerade dabei zu ermitteln, wie viele Server manuell nach der Unterbrechung geprüft werden müssen.

Update: 	24. Mai 2018 14:28:00 CEST 	
	Zwischenzeitlich ist die USV-Schiene wieder im FSN1-DC10 wieder aktiv und es konnte die USV-Absicherung in Teibereichen(einigen Subverteilern) von DC10 wiederhergestellt werden.

Wir arbeiten weiterhin mit Hochdruck an der Wiederherstellung des gesicherten Regelbetriebes.
Update: 	24. Mai 2018 16:36:00 CEST 	
	Zwischenzeitlich wurden alle Server die nicht automatisch nach dem Spannungsabfall wieder angelaufen sind eingeschaltet bzw. auf defekte Netzteile geprüft. Im nächsten Schritt wird dazu übergegangen, die Tickets in den betroffenen Rechenzentren zu bearbeiten - Server die aktuell nicht erreichbar sind, müssen alle individuell geprüft werden, da die Fehlerbilder hier sehr unterschiedlich ausfallen.

Derzeit befinden sich folgende Tickets in den Queues:

DC 07 1013
DC 10 1408
DC 12 465

Um die Bearbeitung nicht weiter zu verzögern bitten wir Sie, lediglich ein Ticket pro betroffenen Server zu erstellen.

Vielen Dank.
Update: 	24. Mai 2018 18:29:00 CEST 	
	Die Analyse der durch die starke Spannungsabsenkung im örtlichen Stromnetz betroffenen und zum Teil beschädigten USV-Anlagen(Modulen)/Subverteilern läuft weiterhin mit Hochdruck.

Zum Teil können betroffene und defekte USV-Module/Subverteiler mit vorhandenen Ersatzteilen durch Techniker des USV-Anlagenherstellers repariert werden, andere Ersatzteile befinden sich im Zulauf und wir arbeiten weiterhin mit Hochdruck an der Wiederherstellung des gesicherten Regelbetriebes. Genauere Informationen wann der gesicherte Regelbetrieb in den betroffen Rechenzentren 7/10/12 wieder hergestellt sein wird, wird sich vermutlich erst morgen abzeichen. Bis dahin befinden sich die betroffenen Rechenzentren ohne oder nur in Teibereichen(einigen Subverteilern) in USV-Absicherung.

Vielen Dank für Ihr Verständnis.
```

# Mein Kommentar dazu

Shit happens.

Hetzner hatte in den letzten Jahren immer mal wieder Probleme mit den USVs.  Kann es Hetzner etwa nicht?

Jein.  Das Problem ist, einen Aufall a la USV kann man nicht planen und auch nicht testen.  Ich vermute, dass die Darstellung von Hetzner korrekt ist:

a) Es gibt eine redundante USV-Versorgung, die 15 Minuten puffern kann.
b) Die USVs selbst sind nochmals N+1-redundant, d. h. verfügen über mehr Module als notwendig.
c) Es handelt sich um besonders energieeffiziente USVs, die es eben nicht von-der-Stange gibt.
d) Die USV-Anlagen stammen nicht von Hetzner selbst, sonderen einem Zulieferer.

Einen Kritikpunkt aber gibt es durchaus:

Es ist nur eine einfache Redundanz der USVs vorhanden.  Bei einem Hochsicherheits- und Hochverfügbarkeitsrechenzentrum (welches Hetzner nicht ist!) würde ich eine mehrfach-redundante USV-Anlage erwarten, d. h. nicht doppelt, sondern dreifach, wobei eine in einer anderen Technik ausgelegt ist (damit genau das Problem hier nicht passiert).  Das Problem in dem Fall ist aber noch immer die Steuerungstechnik d. h. die Erkennung der Störung so rechtzeitig, dass die Redundanz zuschlägt, bevor der Server einen Brownout bekommt.

Welche Ausfalltypen bei USV-Anlagen gibt es?

- Brownout.  Das ist die kuzzeitige Absenkung der Leistung, meist nur über wenige Halbwellen.  Ein typisches Schaltnetzteil kann eine fehlende Halbwelle ausgleichen.

- Spannungsabfall.  Viele Schaltnetzteile kommen mit Stromabsenkungen um 50% klar.  Typische Schaltnetzteile von Laptops können z. B. schon ab 60V arbeiten, d. h. ob die Eingangsspannung 60V oder 230V oder gar 300V beträgt, das ändert nichts.

- Spannungsausfall.  Das ist der eigentlich typische zu erwartende Fall.  Der Strom geht weg, die USV übernimmt.

- Wartungsfehler.  Der typische Denkansatz ist, der Techniker nimmt eine USV offline und dann fällt der Schraubenschlüssel in den Batteriekasten der zweiten USV, es kommt zum Kurzschluss und Notabschaltung der 2. Linie (deshalb braucht es auch 3 Linien um im Wartungsfall weiterhin eine Redundanz zu garantieren).

- Menschliches Versagen.  Jemand schaltet die USV-Anlagen ab.  Es kann z. B. ein Dokumentationsdefekt vorliegen, eine falsche Verdrahtung usw.

Bei Hetzner handelt es sich nicht um ein Hochsicherheits- und Hochverfügbarkeitsrechenzentrum.  Das sieht man schon daran, dass die Stromversorgung zwar redundant ausgelegt ist (2 getrennte Zuleitungen), diese aber beide zum gleichen Umspannwerk führen.  Genau das wurde Hetzner vermutlich heute zum Verhängnis.  Dazu kommt, dass Hetzner aus Effizienzgründen eben keine eigene Stromversorgung hat, sondern die Server am normalen Stromnetz hängen.  Das hat viele negative Einflüsse.

> Ich skizziere mal, wie ein Hochsicherheits- und Hochverfügbarkeits-RZ aussieht.  Dies tue ich anhand eines Colocation-Centers von KPN (von 2003) sowie eines Hochsicherheits-Colocationcenters von COLT (von 1998).
>
> - Das RZ besteht aus 2 baulich getrennten Häusern:  Dem eigentlichen RZ und einem Versorgungsgebäude.
> - Das RZ ist erdbebensicher aufgebaut, d. h. das Gebäude selbst ruht auf Schockabsorbern.
> - Das RZ hat 2 Hüllen.  Die luftige Außenhaut hält z. B. Trucks auf, die mit einer Bombe ins Gebäude fahren wollen.
> - Die Innenhaut besteht aus meterdickem Beton und ist so stark gepanzert, dass die abgehaltene Bombe keinen größeren Schaden anrichten kann.
> - Das Gebäude könnte vermutlich sogar eine mit einem konventionellen Sprengkopf versehenen Raketenangriff überstehen.  Lediglich militärische bunkerbrechnede Angriffe stellen eine Gefahr dar.
> - Durch die Innenhülle kommt man nur durch Personenschleusen mit 2 Panzertüren, die nur wechselseitig öffnen.
> - Im Panikfall (Flucht wenn z. B. die Brandschutzanlage aktiv wurde) kann man die Panzertüren von Innen mit einfachem Handdruck öffnen.  Ein gleichzeitiges öffnen der Türen löst mechanisch einen druckluftgetriebenen Alarm aus.
> - Das Versorgungsgebäude ist ungeschützt.  Fällt das Gebäude aus, ist das RZ in der Lage, ca. 1 Tag ohne das Versorgungsgebäude und ohne externe Versorgung zu überleben.
> - Im RZ selbst stehen auf 2 verschiedenen Etagen je 2 Generatoren.
> - Ein 5. Generator steht zusäztlich im Versorgungsgebäude.
> - Jeder Generator ist in der Lage, den gesamten Strom des RZ zu beliefern.
> - Jedes Hosting-Area, es gibt pro Etage davon mehrere, verfügt über eine eigene, redundante USV-Anlage.
> - Jede USV kann in 3 Modi betrieben werden:  Offline, Online und Überbrückt.
> - Hinter der USVs hängen zusätzlich Crossbars, so dass jede USV die beiden redundanten Rack-Stromversorgungen bereitstellen kann.
> - Mindestens 2 der Generatoren sind permanent online und produzieren den Strom des RZ.
> - Die Generatoren werden durch ein 3-stufiges Konzept angetrieben.
> - Die erste Stufe ist ein Elektromotor.
> - Die zweite Stufe ist eine Schwungmasse.
> - Die dritte Stufe ist ein per Sprengsatz gezündeter Diesel.
> - Außerdem werden die Generatoren phasensynchron gesteuert.
> - Die Kühlung wird durch Wasser sichergestellt, das durch 1m dicke Rohre geleitet wird.
> - Selbst bei vollständigem Ausfall der äußeren Kühlung hat das RZ genügend Kaltwasser, dass es mindestens 1h ohne jegliche(!) Einschränkung weiterbetrieben werden kann.
> - Durch entsprechende Maßnahmen ist sichergestellt, dass die kritischere Dienste des RZ ca. 1 Tag lang und äußerst kritische Systeme ca. 1 Woche lang weiterbetrieben werden könnem, selbst dann, wenn alle Außeninfrastruktur zerstört ist.
> - Für das Personal stehen Atemgeräte mit mehreren Stunden (ca. 2 Tage) Sauerstoffversorgung bereit, im Fall dass der Kern mit Brandschutzgas (damals noch Halon, heute wohl eher CO2) geflutet wurde.
> - Ansonsten gibt es aber keine aktive Sicherheitskomponenten (abgesehen von den wenigen Handfeuerwaffen des Wachschutzes).
> - Es gibt aber einiges an passiver Sicherheitstechnik, wie z. B. gravitationsgetriebene Brandschutzschotten in den getrennten Bandabschnitten, die man schließen kann, und die so konstruiert sind, durch Feuer ausgelösten Teilexplosionen möglichst zu widerstehen.  (Einer Bazooka widerstehen die wohl eher nicht, aber gewöhnlich schießt auch keiner mit einer Bazooka auf Brandschutzschotten.)
> - Dazu kommt, dass im Bedarfsfall Kabelkanäle mit Brandschutzschaum (so etwas wie Bauschaum) hermetisch abgedichtet werden können.  Das Zeug ist binnen Minuten meterdick, panzerhart, gasundurchlässig, absolut unbrennbar und schmilzt erst bei Temperaturen über 1000 Grad, die normale Brände normalerweise nicht erreichen.  (Hinweis:  Diese Kabelkanäle sind den Jeffreys-Röhren in Startrek nicht unähnlich - man kann durchkriechen, allerdings behindern die Kabel das deutlich, es ist also bei weitem nicht so aufgeräumt wie in Startrek und einfach nur nackter Beton.)
>
> Solch ein RZ hält einem militärischen Angriff oder dem THW (die schneiden sich mit 5000 Grad binnen Minuten selbst durch meterdicken Beton) natürlich nicht Stand.  Das soll es aber auch nicht.  Aber es ist gut gegen die zu erwartenden Probleme gefeit, wie kurzfristiger oder längerfristiger Ausfall der Netzinfrastruktur, environmentalen Desastern wie Naturkatastrophen sowie "üblichen" terroristischen Anschlägen wie Bombenanschlägen, Überfall oder abstürzendes Kleinflugzeug.  (Anschläge a la 911 sind ja eher nicht "üblich".)

In der Vergangenheit hatte Hetzner bisher - wie ich das sehe - vor allem Ausfälle durch Brownouts, Spannungsausfall und Wartungsfehler.  So wie ich das sehe hat Hetzner 

Der Designfehler - wenn man von solch einem sprechen will - der heute Hetzner Probleme bereitete liegt wohl an der Konstruktion der USV-Anlage und der Stromzuführung.  Das ist aber nicht unbedingt der Fehler von Hetzner, sondern eher der Energieeffizienz und notwendigen Kosteneffizienz geschuldet.  Oder anders gesagt:  Mit den Rahmenparametern, die Hetzner ein Problem bereitet haben, kann ich (als Hetzner-Kunde) sehr gut leben.

- Eine Zuleitung durch ein 2. weiter weg gelegenes Umpannwerk bedeutet wesentlich höhere Energieverluste.
- Eine dauerhafte Selbstversorgung mit Strom (a la Beispiel oben) senkt die Energieeffizienz extrem.

Es ist also mehr als verständlich, dass zwischen Stromnetz und Servern lediglich eine USV-Anlage hängt, die im Bedarfsfall eingreift.

Ja, und genau hier scheint mir das Problem gelegen zu haben.

Die USV-Anlage wäre in der Lage gewesen, die Server mit Strom zu versorgen.  Das hat sich sicherlich in der Vergangenheit getan.

Die USV-Anlage ist auch sicher redundant ausgelegt, d. h. fällt etwas aus, wird automatisch umgeschaltet.  Auch das wird sie in der Vergangenheit schon mehrfach unter Beweis gestellt haben.

Also, wieso hat es diesmal nicht geklappt?

Nach den (spärlichen) Angaben, die ich von Hetzner lese, ergibt sich für mich folgendes Bild:

- Die Stromversorgung fiel nicht aus, sondern es kam zu einer deutlichen Absenkung der Spannung.
- Die USV hat zweifellos versucht das zu kompensieren, also trotz geringerer Spannung die Versorgung aufrecht zu erhalten.
- Dabei steigt aber die Stromstärke in den Primärkreisen stark an.
- Und dadurch kam es zum Versagen der USV.
- Da Komponenten der USV kaputtgegangen sind, gehe ich nicht von einem einfachen Steuerungsfehler aus (die USV ging nicht unsinnigerweise in die Notabschaltung), sondern von einer Regellücke.  Entweder hat die USV den Zustand nicht richtig erkannt oder der Zustand wurde falsch geregelt, so dass Komponenten der USV zerstört wurden.  Wahrscheinlich fehlten der Regelung der USV bestimmte Parameter (wie z. B. Temperaturen bestimmter Baugruppen), so dass sie nicht rechtzeitig auf den reinen Batteriebetrieb umschalten konnte.
- Ich denke einiges davon ist der besonders hohen Effizienz der USV-Anlage geschuldet.  Solche Anlagen sind noch nicht so häufig in Betrieb, dass genug Testfälle vorhanden sind.

Das Verhängnis wäre vermutlich nicht entstanden, wenn es zu einem vollständigen Stromausfall oder einem Anspringen der Dieselaggregate gekommen wäre.  Das Problem war also die Spannungsabsenkung über eine längere Zeit.  Wobei "längere Zeit" für mich sehr unklar ist.

Vermutlich kann Hetzner folgende Maßnahmen vornehmen:

- Wenn die "längere Zeit" ausreicht, um die Dieselaggregate zu starten, wäre die kurzfristige Lösung ein Monitoring der Stromversorgung und ein frühzeitiges Anwerfen der Diesel, bis das Problem mit der USV gelöst ist.

- Wenn nicht, wäre ein rechtzeitiger automatischer Cutoff der Primär-Stromversorgung anzuraten.  Dann übernimmt der Batteriebetrieb bis die Diesel zur Verfügung stehen.  Die Frage ist lediglich, ob dies so einfach zu realisieren ist.  20 MW nimmt man nicht einfach mal so schnell vom Netz.

- Evtl. kann auch der Energieversorger hier helfen.  Energieversorger haben ein detailliertes Monitoring und können in solch einem Fall Hetzner "abwerfen".  Dadurch kommt es zu einem kompletten Stromausfall, was das RZ ja an sich verkraften können müsste.

Langfristig aber ist der Hersteller der USV gefragt.  Dieser muss diese Situation zuverlässig erkennen und passend regeln.  Der Austausch der Komponenten selbst ist hier eindeutig nicht ausreichend, da es offensichtlich zu einem Kaskadenversagen kam, d. h. der Fail inhärent in der Konstruktion der USV zu suchen ist.

Eine USV tauscht man außerdem nicht einfach mal so schnell aus.  Hetzner ist hier also eindeutig auf den Zulieferer angewiesen und kann da nichts tun.

Bitte auch bedenken:  Der Fehler kam sicherlich nicht dadurch vor, weil Hetzner den billigsten Anbieter genommen hat.  Hätte Hetzner das getan, wäre der Fehler vermutlich nicht aufgetreten, weil es sich dann um eine konventionelle - weniger energieeffiziente - USV-Anlage gehandelt hätte.

Das ist eben der Nachteil wenn man auf nichtetablierte modernere und energieeffiziente Technik setzt.  Solche Fehler sind hinzunehemen.

Oder anders gesagt:  Wer einen kritischen Webservice betreibt, und diesen so konstruiert hat, dass er den Aufall eines RZ nicht überleben kann, der hat eh schon im Vorfeld versagt.  Da ist dann nicht der ISP schuld, sondern man selbst.  Man sollte schon sehr deutlich wissen, was man wie konstruiert.

Genau deshalb stehen die meisten meiner Server bei Hetzner!  Das mal der Strom weg ist kann ich verkraften.  Was ich nicht verkraften kann wäre zähflüssiger Service im Fall eines Hardwareproblems, das ich so nicht remote lösen kann.  Aber dass ich nach einem Stromausfall mal schnell Remote Hand an meine Server legen muss, weil bei diesen irgendetwas nach dem Stromausfall nicht richtig hochkam, damit kann ich leben.

Oder genauer:  Wenn etwas nicht wieder von selbst anläuft (so lange keine Hardware defekt ist) ist das **mein Fehler**!  Services sollten immer rebootfest konstruiert sein!  Schließlich muss man ja auch mal irgendwann den Kernel patchen.

In diesem Sinne:  Business as usual.  War eine gute Übung.
